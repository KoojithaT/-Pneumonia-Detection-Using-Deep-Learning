#Downloading Dataset

%%capture

def kaggleSetUp():
  !pip install kaggle
  from google.colab import drive
  drive.mount('/content/drive')
  !mkdir  ~/.kaggle
  !cp /content/drive/MyDrive/Kaggle/Kaggle_Credential/kaggle.json ~/.kaggle/kaggle.json
  !chmod 600 ~/.kaggle/kaggle.json
  !kaggle config --list


kaggleSetUp()
!kaggle datasets download paultimothymooney/chest-xray-pneumonia
!unzip chest-xray-pneumonia.zip

%%capture
!pip install tensorflow
!pip install keras
!pip install --upgrade tensorflow
!pip install keras-preprocessing

#Importing Libraries

import tensorflow as tf
from tensorflow.keras.applications import ResNet50, VGG16
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.data import AUTOTUNE
from tensorflow.keras import layers, models

from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess_input
from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess_input


import os
import numpy as np
import matplotlib.pyplot as plt

import warnings

#Data Exploration

train_dir = "/content/chest_xray/train"
test_dir = "/content/chest_xray/test"
val_dir = "//content/chest_xray/val"

print(os.listdir(train_dir))
print(os.listdir(test_dir))
print(os.listdir(val_dir))

os.listdir(train_dir)[0]

labels = ["PNEUMONIA","NORMAL"]
labels

image_size = (256, 256)
batch_size = 32

train_data = tf.keras.utils.image_dataset_from_directory(
    train_dir,
    image_size=image_size,
    batch_size=batch_size,
    label_mode='binary',
    validation_split=0.2,
    subset='training',
    seed=27
)

val_data = tf.keras.utils.image_dataset_from_directory(
    train_dir,
    image_size=image_size,
    batch_size=batch_size,
    label_mode='binary',
    validation_split=0.2,
    subset='validation',
    seed=27
)

test_data = tf.keras.utils.image_dataset_from_directory(
    test_dir,
    image_size=image_size,
    batch_size=batch_size,
    label_mode='binary'
)

class_names = train_data.class_names


plt.figure(figsize=(10, 10))
for images, labels in train_data.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[int(labels[i])])
        plt.axis("off")

def plot_class_distribution(dataset, class_names, dataset_name='Dataset'):
    class_count = np.zeros(len(class_names))


    for _, labels in dataset:
        labels = labels.numpy()
        for label in labels:
            class_count[int(label)] += 1


    plt.figure(figsize=(8, 6))
    plt.bar(class_names, class_count)
    plt.title(f'{dataset_name} Class Distribution')
    plt.ylabel('Number of Samples')
    plt.xlabel('Class')
    plt.show()


plot_class_distribution(train_data, class_names, dataset_name='Training Set')
plot_class_distribution(val_data, class_names, dataset_name='Validation Set')
plot_class_distribution(test_data, class_names, dataset_name='Test Set')

from tensorflow.keras import layers
from tensorflow.keras import Sequential


data_augmentation = Sequential([
    layers.RandomRotation(factor=0.05),
    layers.RandomZoom(height_factor=0.1, width_factor=0.1),
    layers.RandomTranslation(height_factor=0.1, width_factor=0.1),
    layers.RandomFlip(mode='horizontal'),
    layers.RandomContrast(factor=0.1),
    layers.Resizing(256, 256)
])

train_data_augmented = train_data.map(lambda x, y: (data_augmentation(x, training=True), y))

train_data_augmented = train_data_augmented.prefetch(buffer_size=AUTOTUNE)


val_data_prefetched = val_data.prefetch(buffer_size=AUTOTUNE)
test_data_prefetched = test_data.prefetch(buffer_size=AUTOTUNE)

def plot_augmented_examples(dataset, num_images=9):
    plt.figure(figsize=(10, 10))
    for images, labels in dataset.take(1):
        images = images.numpy()
        labels = labels.numpy()

        for i in range(min(num_images, len(images))):
            ax = plt.subplot(3, 3, i + 1)
            plt.imshow(images[i].astype("uint8"))
            plt.axis("off")
    plt.show()

#Data Preprocessing

def preprocess_vgg(image, label):
    image = vgg_preprocess_input(image)
    return image, label

def preprocess_resnet(image, label):
    image = resnet_preprocess_input(image)
    return image, label

def preprocess_pneumonia_net(image, label):
    image = tf.cast(image, tf.float32) / 255.0
    return image, label

train_data_vgg = train_data_augmented.map(preprocess_vgg)
val_data_vgg = val_data.map(preprocess_vgg)
test_data_vgg = test_data.map(preprocess_vgg)


train_data_resnet = train_data_augmented.map(preprocess_resnet)
val_data_resnet = val_data.map(preprocess_resnet)
test_data_resnet = test_data.map(preprocess_resnet)

train_data_pneumonia = train_data_augmented.map(preprocess_pneumonia_net)
val_data_pneumonia = val_data.map(preprocess_pneumonia_net)
test_data_pneumonia = test_data.map(preprocess_pneumonia_net)

#Baseline Models Testing

#VGG16

def create_vgg16_model(input_shape=(256, 256, 3)):
    base_model_vgg = VGG16(weights='imagenet',
                           include_top=False,
                           input_shape=input_shape)


    base_model_vgg.trainable = False
    inputs = layers.Input(shape=input_shape)
    x = base_model_vgg(inputs, training=False)
    x = layers.GlobalAveragePooling2D()(x)
    outputs = layers.Dense(1)(x)
    model_vgg = Model(inputs, outputs)
    model_vgg.compile(optimizer=Adam(learning_rate = 1e-3),
                      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                      metrics=[
                        'accuracy',
                        tf.keras.metrics.Precision(name='precision'),
                        tf.keras.metrics.Recall(name='recall')
                    ])

    return model_vgg

def train_and_evaluate(model, train_dataset, val_dataset, epochs=5):

    history = model.fit(train_dataset,
                        validation_data=val_dataset,
                        epochs=epochs,
                        verbose=1)

    return history

#Training VGG16

model_vgg = create_vgg16_model()
history_vgg = train_and_evaluate(model_vgg, train_data_vgg, val_data_vgg)

def plot_history(history, title):
    plt.figure(figsize=(18, 12))
    plt.subplot(2, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Train Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title(f'{title} Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()


    plt.subplot(2, 2, 2)
    plt.plot(history.history.get('loss', []), label='Train Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title(f'{title} Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()


    plt.subplot(2, 2, 3)
    plt.plot(history.history.get('precision', []), label='Train Precision')
    plt.plot(history.history.get('val_precision', []), label='Validation Precision')
    plt.title(f'{title} Precision')
    plt.xlabel('Epoch')
    plt.ylabel('Precision')
    plt.legend()


    plt.subplot(2, 2, 4)
    plt.plot(history.history.get('recall', []), label='Train Recall')
    plt.plot(history.history.get('val_recall', []), label='Validation Recall')
    plt.title(f'{title} Recall')
    plt.xlabel('Epoch')
    plt.ylabel('Recall')
    plt.legend()

    plt.tight_layout()
    plt.show()

plot_history(history_vgg, 'VGG16')

def evaluate_model(model, dataset, specified_set):

    dataset_loss, dataset_accuracy, dataset_precision, dataset_recall = model.evaluate(dataset, verbose=0)


    print(f"{specified_set} Loss: {dataset_loss}")
    print(f"{specified_set} Accuracy: {dataset_accuracy}")
    print(f"{specified_set} Precision: {dataset_precision}")
    print(f"{specified_set} Recall: {dataset_recall}")

evaluate_model(model_vgg, train_data_vgg, 'Train')

evaluate_model(model_vgg, val_data_vgg, 'Validation')

model_vgg.summary()

tf.keras.utils.plot_model(model_vgg, show_shapes=True)

from tensorflow.keras.models import load_model

model_vgg.save('/content/drive/MyDrive/SAVED MODELS/model_vgg.h5')

from tensorflow.keras.models import load_model

model = load_model("/content/drive/MyDrive/SAVED MODELS/model_vgg.h5")

from keras.preprocessing import image
import numpy as np
from keras.applications.vgg16 import preprocess_input
from keras.activations import sigmoid

def predict_pneumonia(model, img_path, target_size=(256, 256)):

    img = image.load_img(img_path, target_size=target_size)
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    img_data = preprocess_input(x)


    classes = model.predict(img_data)
    probability = sigmoid(classes[0][0])


    prediction = 1 if probability > 0.5 else 0


    print(f"Predicted Probability: {probability}")
    print(f"Predicted Class: {prediction}")

    return probability, prediction

img_path = "/content/chest_xray/chest_xray/val/PNEUMONIA/person1950_bacteria_4881.jpeg"
probability, prediction = predict_pneumonia(model_vgg, img_path)

img_path = "/content/chest_xray/chest_xray/val/NORMAL/NORMAL2-IM-1442-0001.jpeg"
probability, prediction = predict_pneumonia(model_vgg, img_path)

def create_resnet50_model(input_shape=(256, 256, 3)):
    base_model_resnet = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)
    base_model_resnet.trainable = False
    inputs = layers.Input(shape=input_shape)
    x = base_model_resnet(inputs, training=False)
    x = layers.GlobalAveragePooling2D()(x)
    outputs = layers.Dense(1)(x)
    model_resnet = Model(inputs, outputs)
    model_resnet.compile(optimizer=Adam(learning_rate = 1e-3),
                         loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                         metrics=[
                            'accuracy',
                            tf.keras.metrics.Precision(name='precision'),
                            tf.keras.metrics.Recall(name='recall')
                         ])

    return model_resnet

#Training ResNet50

model_resnet = create_resnet50_model()
history_resnet = train_and_evaluate(model_resnet, train_data_resnet, val_data_resnet)

plot_history(history_resnet, 'ResNet50')

evaluate_model(model_resnet, train_data_resnet, 'Train')

evaluate_model(model_resnet, val_data_resnet, 'Validation')

model_resnet.summary()

tf.keras.utils.plot_model(model_vgg, show_shapes=True)

from tensorflow.keras.models import load_model

model_resnet.save('/content/drive/MyDrive/SAVED MODELS/model_resnet.h5')

from tensorflow.keras.models import load_model

model = load_model("/content/drive/MyDrive/SAVED MODELS/model_resnet.h5")

img_path = "/content/chest_xray/chest_xray/val/PNEUMONIA/person1950_bacteria_4881.jpeg"
probability, prediction = predict_pneumonia(model_resnet, img_path)

img_path = "/content/chest_xray/chest_xray/val/NORMAL/NORMAL2-IM-1442-0001.jpeg"
probability, prediction = predict_pneumonia(model_resnet, img_path)

#PneumoniaNet

from tensorflow.keras import layers, Sequential
from tensorflow.keras.optimizers import Adam

def PneumoniaNet():

    model = Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(256, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(512, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(1, activation='sigmoid')
    ])


    model.compile(optimizer=Adam(learning_rate=1e-3),
                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),
                  metrics=[
                      'accuracy',
                      tf.keras.metrics.Precision(name='precision'),
                      tf.keras.metrics.Recall(name='recall')
                  ])

    return model

def train_and_evaluate(model, train_dataset, val_dataset, epochs=5):
    history = model.fit(train_dataset,
                        validation_data=val_dataset,
                        epochs=epochs,
                        verbose=1)

    return history

model = PneumoniaNet()
history_pneumonia = train_and_evaluate(model, train_data_pneumonia, val_data_pneumonia)

plot_history(history_pneumonia, 'PneumoniaNet')

evaluate_model(model, train_data_pneumonia, 'Train')

evaluate_model(model, val_data_pneumonia, 'Validation')

model.summary()

tf.keras.utils.plot_model(model, show_shapes=True)

from tensorflow.keras.models import load_model

model.save('/content/drive/MyDrive/SAVED MODELS/model_pneumoniaNet.h5')

from tensorflow.keras.models import load_model

model = load_model("/content/drive/MyDrive/SAVED MODELS/model_pneumoniaNet.h5")

img_path = "/content/chest_xray/chest_xray/val/PNEUMONIA/person1950_bacteria_4881.jpeg"
probability, prediction = predict_pneumonia(PneumoniaNet(), img_path)

img_path = "/content/chest_xray/chest_xray/val/NORMAL/NORMAL2-IM-1442-0001.jpeg"
probability, prediction = predict_pneumonia(PneumoniaNet(), img_path)



